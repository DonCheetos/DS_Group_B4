{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705e05ac-b189-4d39-a4bf-fa95e22e1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af4cee-c612-4608-a98c-eeebf465006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables:        name\n",
      "0  metadata\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database (since it's in the same folder, you can just use the filename)\n",
    "db_path = 'metadata-110mil.sqlite'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Check available tables in the database (optional)\n",
    "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "print(\"Available tables:\", tables)\n",
    "\n",
    "# Specify the table you want to load\n",
    "table_name = 'metadata'  # Replace with your actual table name\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {table_name};\", conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82eec0c-686e-44e6-ad6d-1bd8a7b297c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.iloc[0]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb74d1d-2c77-4540-b71a-ff8be16003aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Set the font to a universal one like Noto Sans\n",
    "#rcParams['font.sans-serif'] = ['Noto Sans', 'DejaVu Sans', 'Arial Unicode MS']  # Ensure Noto Sans is installed\n",
    "#rcParams['axes.unicode_minus'] = False  # Avoid issues with minus signs\n",
    "\n",
    "# Load your data (assuming it's already in a DataFrame `df`)\n",
    "# df = pd.read_csv('your_file.csv')  # Uncomment this if you're loading from a CSV file\n",
    "\n",
    "# Split categories by commas and stack them to get a single column of all categories\n",
    "#all_categories = df['Category'].str.split(',').explode()\n",
    "\n",
    "# Strip any extra whitespace from each category (important if there are spaces after commas)\n",
    "#all_categories = all_categories.str.strip()\n",
    "\n",
    "# Count occurrences of each unique category\n",
    "#category_counts = all_categories.value_counts()\n",
    "\n",
    "# Filter categories to include only those with at least 500 occurrences\n",
    "#filtered_category_counts = category_counts[category_counts >= 10000]\n",
    "\n",
    "# Plot as a bar chart\n",
    "#plt.figure(figsize=(20, 6))\n",
    "#filtered_category_counts.plot(kind='bar', color='skyblue')\n",
    "#plt.title('Distribution of Categories (at least 10000 entries)')\n",
    "#plt.xlabel('Category')\n",
    "#plt.ylabel('Count')\n",
    "#plt.xticks(rotation=90, ha='right')\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf1a21-4d70-4507-a199-b889e58acaed",
   "metadata": {},
   "source": [
    "![category_distribution](category_distribution.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897ab0c-2cc2-4dca-a185-af9906d05f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Take a random sample of 100,000 rows (adjust the number as needed for performance)\n",
    "#sample_df = df['word_count'].sample(100000, random_state=42)\n",
    "\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.histplot(sample_df, bins=50, kde=True)\n",
    "\n",
    "#plt.xlabel(\"Word Count\")\n",
    "#plt.ylabel(\"Frequency\")\n",
    "#plt.title(\"Distribution of Word Count (Sampled)\")\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbce96-bdb5-428c-9006-22a004697755",
   "metadata": {},
   "source": [
    "![word distribution](word_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a3bee-5f6f-42ae-87eb-5748c03fa81c",
   "metadata": {},
   "source": [
    "<h2> Cleaning and Formating Data: </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4d501-9a02-4e6d-b7c7-c6f366c720b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff692b-9520-4f26-baa2-cda33fa708b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_all_values = df[df[\"Publisher\"] == \"\"].value_counts()#.index.tolist()\n",
    "len(missing_all_values) # TODO: Clean all that are missing. 79 missing is rows that dont hae any value beside path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8443e-0960-42fe-a28a-d911ea5cfadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_null = (df.isnull()).sum()\n",
    "rows_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e9f10-86f4-4b65-a1e6-eb9bf5357afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_missing_values = (df.isnull() | (df == \"\")).any(axis=1).sum()\n",
    "rows_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220c411-5a44-4f87-ae7d-078a30c071f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_missing_values / len(df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18db40-7a06-466c-bf7a-2a130df9d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing = (df == \"\").sum()\n",
    "all_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dfa95-31cc-43f1-a527-0f1ca7dfa157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing data type for chapter and word count into int type\n",
    "\n",
    "df['word_count'] = pd.to_numeric(df['word_count'], errors='coerce').astype('Int64')\n",
    "df['chapter_count'] = pd.to_numeric(df['chapter_count'], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0cef3-4dfa-48d8-ba62-40331a3540f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_usable = df.copy(deep=True) \n",
    "\n",
    "# Remove redundant  and unimportant columns\n",
    "\n",
    "df_usable = df.drop(columns=['Chapters', 'Words','Path','Story URL','Author URL'])\n",
    "\n",
    "# remove rows where all values are missing (79 of those)\n",
    "\n",
    "df_usable = df_usable[df_usable['word_count'] != '']\n",
    "\n",
    "# All where summary, genre and category is missing\n",
    "\n",
    "df_usable = df_usable[df_usable['Summary'] != '']\n",
    "df_usable = df_usable[df_usable['Category'] != '']\n",
    "df_usable = df_usable[df_usable['Genre'] != '']\n",
    "\n",
    "# Removing extreme word count values  (100 < x < 2,000,000)\n",
    "\n",
    "df_usable = df_usable[(df_usable['word_count'] > 100) & (df_usable['word_count'] < 2000000)]\n",
    "df_usable=df_usable[(df_usable['Language'] == 'English')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0c06a-59d6-451f-85d3-38c2eea4b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting after cleaning all the missing\n",
    "missing = (df_usable == \"\").sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998fc4d-8994-457e-ab7d-ed9821a92614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8508e1-9796-4981-913c-7bb31ff42e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_usable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae97e2-f0a9-4f0b-8337-e633f7691664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizer data\n",
    "nltk.download('wordnet')  # Lemmatizer data\n",
    "nltk.download('stopwords')  # Stopwords data\n",
    "nltk.download('omw-1.4')  # WordNet data\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4ece2-7d87-4508-bba6-3763416b36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stop_words = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabet characters\n",
    "    text = text.lower()  # Lowercase text\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stop_words]  # Lemmatize and remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to Title and Summary\n",
    "df_usable['processed_title'] = df_usable['Title'].apply(preprocess_text)\n",
    "df_usable['processed_summary'] = df_usable['Summary'].apply(preprocess_text)\n",
    "\n",
    "# Combine title and summary\n",
    "df_usable['combined_text'] = df_usable['processed_title'] + df_usable['processed_summary']\n",
    "\n",
    "# Group by Category and Genre\n",
    "grouped_data = df_usable.groupby(['Category', 'Genre'])\n",
    "\n",
    "# Collect unique words for each group\n",
    "unique_words_by_group = defaultdict(list)\n",
    "\n",
    "for (category, genre), group in grouped_data:\n",
    "    all_words = [word for text in group['combined_text'] for word in text]\n",
    "    unique_words = set(all_words)  # Find unique words\n",
    "    unique_words_by_group[(category, genre)] = unique_words\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "unique_words_df = pd.DataFrame([\n",
    "    {'Category': k[0], 'Genre': k[1], 'Unique_Words': list(v)} \n",
    "    for k, v in unique_words_by_group.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9155f86-39db-4172-b9fd-84c381230dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679ca18-c59e-44c6-982c-803a4d136875",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_df[unique_words_df[\"Unique_Words\"].apply(len) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656df33-e80c-433b-8c3f-2e200e04a7ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_usable[df_usable[\"Category\"].str.contains(\"Harry\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f993f31f-e29d-447e-b2d6-2115897e1c12",
   "metadata": {},
   "source": [
    "<h2> Build a model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec761f86-5b5d-4ad6-a30a-90a320ac3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdec4a-b3d6-4ffe-b15f-bf991149b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable[\"word_count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f923898-ec29-42a3-ab53-18df48ef520c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1b90f-ae00-4cad-a51e-a2ca268406fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df[['word_count', 'chapter_count']].copy()\n",
    "\n",
    "# Convert these columns to numeric (if needed)\n",
    "numerical_columns = numerical_columns.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop any rows with NaN values in numerical columns to avoid calculation issues\n",
    "numerical_columns = numerical_columns.dropna()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.matshow(correlation_matrix, cmap='coolwarm', fignum=1)\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Matrix\", pad=20)\n",
    "plt.show()\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f51f3e-5843-4c04-810f-ba9e265e9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d130c-0665-4c77-9601-211462d2f1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['word_count'] = pd.to_numeric(df['word_count'], errors='coerce').astype('Int64')\n",
    "df[df[\"word_count\"] > 2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f522f-205e-48e9-a8c9-c1ac9f794c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5568444][\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a8b2e-0269-4b02-8308-ddfb30df480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf429ed6-d18c-47d0-8a89-25828ac7e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(all_categories.unique()))#9975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac0593-0bf6-49df-9277-c79a56a37311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[12430]['Summary']\n",
    "df.iloc[12430]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd8db6-5037-4ab1-b0e0-a9013b857b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences in the \"Category\" column\n",
    "category_counts = df_usable['Category'].value_counts()\n",
    "valid_categories = category_counts[category_counts >= 100].index\n",
    "df_filtered = df_usable[df_usable['Category'].isin(valid_categories)]\n",
    "df_usable=df_filtered\n",
    "print(len(df_usable['Category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb11a1-f044-41d5-95b8-90388fdd6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable.head()\n",
    "df_usable = df_usable.drop(columns=['Packaged', 'Publisher'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a84dc-eaed-413f-91a4-73ef6ae611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable.shape\n",
    "# Specify the output file\n",
    "output_file = \"filtered_usable_df.csv\"\n",
    "\n",
    "# Save to CSV\n",
    "df_usable.to_csv(output_file, sep=\";\", encoding=\"utf-8\", index=False)  # Set index=False to avoid saving the index\n",
    "\n",
    "print(f\"DataFrame successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a0c59-8abc-42ea-b268-6135e0b06d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable2 = pd.read_csv(\"filtered_usable_df.csv\", sep=\";\")\n",
    "print(df_usable2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4366a05-cf34-4a23-8478-297d2e2aea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usable.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
